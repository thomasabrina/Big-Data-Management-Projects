{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6565e3f8",
   "metadata": {},
   "source": [
    "## Project Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55974225",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from delta import *\n",
    "from delta.tables import *\n",
    "import os\n",
    "import uuid\n",
    "import time\n",
    "\n",
    "builder = SparkSession.builder.appName(\"Streaming2_practice\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.sql.streaming.statefulOperator.checkCorrectness.enabled\", \"false\") \\\n",
    "    .config(\"spark.jars.packages\", 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0') \\\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"false\") \\\n",
    "    .config(\"spark.sql.debug.maxToStringFields\", \"10000\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate() # type: ignore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e622d5b-ef4d-4d78-abcb-e9edc2305ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, FloatType\n",
    "\n",
    "schema = StructType(\n",
    "      [\n",
    "        StructField(\"hack_license\", StringType(), False),\n",
    "        StructField(\"pick_up_location\", StringType(), False),\n",
    "        StructField(\"drop_off_location\", StringType(), False),\n",
    "        StructField(\"pick_up_time\", TimestampType(), False),\n",
    "        StructField(\"drop_off_time\", TimestampType(), False),\n",
    "        StructField(\"timestamp\", TimestampType(), False),\n",
    "      ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69712d38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brokers = os.getenv('KAFKA_ADVERTISED_LISTENERS')\n",
    "protocol = os.getenv('KAFKA_LISTENER_SECURITY_PROTOCOL_MAP') \n",
    "\n",
    "lines = (spark.readStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", brokers)\n",
    "    .option(\"kafka.security.protocol\", protocol)\n",
    "    .option(\"subscribe\", \"stock\")\n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .option(\"maxOffsetsPerTrigger\", 100)\n",
    "    .load()\n",
    ")\n",
    "\n",
    "parsed_lines = lines.select(F.from_json(lines.value.cast(\"string\"), schema).alias(\"data\")).select(\"data.*\")\n",
    "parsed_lines.writeStream.format(\"console\").start().awaitTermination(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24187ef-e5b4-4fa7-bab5-60aa94412a05",
   "metadata": {},
   "source": [
    "## [Query 1] Utilization over a window of 5, 10, and 15 minutes per taxi/driver. This can be computed by computing the idle time per taxi. How does it change? Is there an optimal window?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7f6731",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65b2d7ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'parsed_lines' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m output_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m watermark_duration \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m30 minutes\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 8\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mparsed_lines\u001b[49m\u001b[38;5;241m.\u001b[39mwithWatermark(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrop_off_time\u001b[39m\u001b[38;5;124m\"\u001b[39m, watermark_duration)  \u001b[38;5;66;03m# Set a watermark to allow for late data up to an hour\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Apply window functions\u001b[39;00m\n\u001b[1;32m     11\u001b[0m windows \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mgroupBy(\n\u001b[1;32m     12\u001b[0m     F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhack_license\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     13\u001b[0m     F\u001b[38;5;241m.\u001b[39mwindow(F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrop_off_time\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m5 minutes\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     14\u001b[0m     F\u001b[38;5;241m.\u001b[39mwindow(F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrop_off_time\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m10 minutes\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     15\u001b[0m     F\u001b[38;5;241m.\u001b[39mwindow(F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrop_off_time\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m15 minutes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m );\n",
      "\u001b[0;31mNameError\u001b[0m: name 'parsed_lines' is not defined"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "checkpoint_path = \"./checkpoint\" \n",
    "output_path = \"output/\"\n",
    "watermark_duration = \"30 minutes\"\n",
    "\n",
    "\n",
    "# Set up queries for each window duration\n",
    "queries = []\n",
    "windows = [5, 10, 15]\n",
    "for duration in windows:\n",
    "    window_duration = f\"{duration} minutes\"\n",
    "\n",
    "    df = parsed_lines.withWatermark(\"pick_up_time\", watermark_duration)\\\n",
    "        .withColumn(\"active_time\", F.unix_timestamp(\"drop_off_time\") - F.unix_timestamp(\"pick_up_time\"));\n",
    "\n",
    "    df.writeStream.format(\"console\").start().awaitTermination(5)\n",
    "\n",
    "\n",
    "    df_grouped = df.groupBy(\n",
    "        F.col(\"hack_license\"), \n",
    "        F.window(F.col(\"pick_up_time\"), window_duration)\n",
    "    ).agg(\n",
    "        F.sum(\"active_time\").alias(\"total_active_time\"),\n",
    "        F.min(\"pick_up_time\").alias(\"first_pick_up_time\"),\n",
    "        F.max(\"drop_off_time\").alias(\"last_drop_off_time\")\n",
    "    )\n",
    "\n",
    "    df_grouped.writeStream.format(\"console\").start().awaitTermination(5)\n",
    "\n",
    "\n",
    "    # Calculate the idle time\n",
    "    idle_time_df = df_grouped.withColumn(\"idle_time\", \\\n",
    "                                         (F.unix_timestamp(F.col(\"last_drop_off_time\")) - F.unix_timestamp(F.col(\"first_pick_up_time\"))) - F.col(\"total_active_time\"))\n",
    "\n",
    "    idle_time_df.writeStream.format(\"console\").start().awaitTermination(5)\n",
    "\n",
    "\n",
    "    query = idle_time_df.writeStream \\\n",
    "            .outputMode(\"append\") \\\n",
    "            .format(\"delta\") \\\n",
    "            .trigger(processingTime=\"120 seconds\") \\\n",
    "            .option(\"checkpointLocation\", f\"{checkpoint_path}/duration_{duration}_min\") \\\n",
    "            .option(\"path\", f\"{output_path}/duration_{duration}_min\") \\\n",
    "            .option(\"mergeSchema\", \"true\") \\\n",
    "            .start()\n",
    "    queries.append(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88a5bafe-a7fc-4b9d-810a-7eee94624b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "for query in queries:\n",
    "    query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1ea15f2-e119-4a8a-94a6-901576e9e8da",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_NOT_FOUND] Path does not exist: output/duration_5_min.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelta\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mload(path)\u001b[38;5;241m.\u001b[39mtoPandas()\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Collect data from different window outputs\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m data_frames \u001b[38;5;241m=\u001b[39m \u001b[43m{\u001b[49m\u001b[43mduration\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mread_delta_table\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43moutput_path\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/duration_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mduration\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_min\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mwindows\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(data_frames)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# for duration, df in data_frames.items():\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#     df['window_size'] = f\"{duration}min\"  # Add window size as a string, e.g., '5min'\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#     df['window_start'] = pd.to_datetime(df['window'].str.extract(r'\\((.*),')[0])  # Optional: Extract start of window for any further temporal analysis\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# plt.legend(title='Window size')\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# plt.show()\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 13\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelta\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mload(path)\u001b[38;5;241m.\u001b[39mtoPandas()\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Collect data from different window outputs\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m data_frames \u001b[38;5;241m=\u001b[39m {duration: \u001b[43mread_delta_table\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43moutput_path\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/duration_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mduration\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_min\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m duration \u001b[38;5;129;01min\u001b[39;00m windows}\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(data_frames)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# for duration, df in data_frames.items():\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#     df['window_size'] = f\"{duration}min\"  # Add window size as a string, e.g., '5min'\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#     df['window_start'] = pd.to_datetime(df['window'].str.extract(r'\\((.*),')[0])  # Optional: Extract start of window for any further temporal analysis\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# plt.legend(title='Window size')\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# plt.show()\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 10\u001b[0m, in \u001b[0;36mread_delta_table\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_delta_table\u001b[39m(path):\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdelta\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtoPandas()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py:307\u001b[0m, in \u001b[0;36mDataFrameReader.load\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: output/duration_5_min."
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "checkpoint_path = \"./checkpoint\" \n",
    "output_path = \"output/\"\n",
    "windows = [5, 10, 15]\n",
    "\n",
    "# Assuming you have a function to read delta tables into a dataframe\n",
    "def read_delta_table(path):\n",
    "    return spark.read.format(\"delta\").load(path).toPandas()\n",
    "\n",
    "# Collect data from different window outputs\n",
    "data_frames = {duration: read_delta_table(f\"{output_path}/duration_{duration}_min\") for duration in windows}\n",
    "print(data_frames)\n",
    "\n",
    "# for duration, df in data_frames.items():\n",
    "#     df['window_size'] = f\"{duration}min\"  # Add window size as a string, e.g., '5min'\n",
    "#     df['window_start'] = pd.to_datetime(df['window'].str.extract(r'\\((.*),')[0])  # Optional: Extract start of window for any further temporal analysis\n",
    "\n",
    "# # Combine all data frames into one\n",
    "# combined_df = pd.concat(data_frames.values(), ignore_index=True)\n",
    "\n",
    "# # Drop the 'window' column if no longer needed\n",
    "# combined_df.drop('window', axis=1, inplace=True)\n",
    "\n",
    "# # Pivot the DataFrame\n",
    "# pivot_df = combined_df.pivot_table(index='hack_licence', columns='window_size', values='idle_time', aggfunc='mean')\n",
    "# print(pivot_df)\n",
    "\n",
    "# # Optional: Reset the index if you want 'hack_licence' as a regular column for plotting\n",
    "# pivot_df.reset_index(inplace=True)\n",
    "\n",
    "# # Example of plotting\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# pivot_df.set_index('hack_licence').plot(kind='bar', figsize=(10, 7))\n",
    "# plt.title('Average Idle Time per Window Size')\n",
    "# plt.xlabel('Driver')\n",
    "# plt.ylabel('Average Idle Time (seconds)')\n",
    "# plt.xticks(rotation=0)\n",
    "# plt.legend(title='Window size')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6746caef-fc7c-4d0e-98df-cdd6046393eb",
   "metadata": {},
   "source": [
    "## [Query 2] The average time it takes for a taxi to find its next fare(trip) per destination borough. This can be computed by finding the time difference, e.g. in seconds, between the trip's drop off and the next trip's pick up within a given unit of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bea404b-fc76-48f9-83d9-5946617863de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember you can register another stream\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8268c285-d55c-4be3-8e5a-e7ddebb14153",
   "metadata": {},
   "source": [
    "## [Query 3] The number of trips that started and ended within the same borough in the last hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e7ad21-59c2-4d4c-befe-9d1ceedbb74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember you can register another stream\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d0d685-c3ed-4b7d-8ebc-c3174ba55645",
   "metadata": {},
   "source": [
    "## [Query 4] The number of trips that started in one borough and ended in another one in the last hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f3578b-1960-4969-801b-adc2f45493a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
