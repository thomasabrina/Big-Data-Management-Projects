{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6565e3f8",
   "metadata": {},
   "source": [
    "## Project Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55974225",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from delta import *\n",
    "from delta.tables import *\n",
    "import os\n",
    "import uuid\n",
    "import time\n",
    "\n",
    "builder = SparkSession.builder.appName(\"Streaming2_practice\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.sql.streaming.statefulOperator.checkCorrectness.enabled\", \"false\") \\\n",
    "    .config(\"spark.jars.packages\", 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0') \\\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True)\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e622d5b-ef4d-4d78-abcb-e9edc2305ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, BooleanType, TimestampType, DateType\n",
    "\n",
    "schema = StructType(\n",
    "      [\n",
    "        StructField(\"hack_licence\", StringType(), False),\n",
    "        StructField(\"pickup_location\", StringType(), False),\n",
    "        StructField(\"pickup_location\", StringType(), False),\n",
    "        StructField(\"pick_up_time\", StringType(), False),\n",
    "        StructField(\"drop_off_time\", StringType(), False),\n",
    "        StructField(\"timestamp\", TimestampType(), False),\n",
    "      ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69712d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "brokers = os.getenv('KAFKA_ADVERTISED_LISTENERS')\n",
    "protocol = os.getenv('KAFKA_LISTENER_SECURITY_PROTOCOL_MAP') \n",
    "from pyspark.sql.functions import from_json\n",
    "\n",
    "lines = (spark.readStream                        \n",
    "  .format(\"kafka\")                                 \n",
    "  .option(\"kafka.bootstrap.servers\", brokers) \n",
    "  .option(\"kafka.security.protocol\", protocol)\n",
    "  .option(\"subscribe\", \"stock\")                       \n",
    "  .option(\"startingOffsets\", \"earliest\")          \n",
    "  .option(\"maxOffsetsPerTrigger\", 100)             \n",
    "  .load()\n",
    ")\n",
    "\n",
    "parsed_lines = lines.select(from_json(lines.value.cast(\"string\"), schema).alias(\"data\")).select(\"data.*\") \\\n",
    "    .withColumn(\"pick_up_time\", F.to_timestamp(\"pick_up_time\")) \\\n",
    "    .withColumn(\"drop_off_time\", F.to_timestamp(\"drop_off_time\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24187ef-e5b4-4fa7-bab5-60aa94412a05",
   "metadata": {},
   "source": [
    "## [Query 1] Utilization over a window of 5, 10, and 15 minutes per taxi/driver. This can be computed by computing the idle time per taxi. How does it change? Is there an optimal window?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65b2d7ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import TimestampType\n",
    "from datetime import datetime\n",
    "\n",
    "checkpoint_path = \"./checkpoint\" \n",
    "output_path = \"output/\"\n",
    "watermark_duration = \"30 minutes\"\n",
    "\n",
    "df = parsed_lines.withColumn(\"timestamp\", F.col(\"timestamp\").cast(TimestampType())).withWatermark(\"timestamp\", watermark_duration)\n",
    "\n",
    "\n",
    "# Set up queries for each window duration\n",
    "queries = []\n",
    "windows = [5, 10, 15]\n",
    "for duration in windows:\n",
    "    window_duration = f\"{duration} minutes\"\n",
    "\n",
    "\n",
    "    # Calculate the duration of each trip\n",
    "    df = df.withColumn(\"active_time\", F.unix_timestamp(\"drop_off_time\") - F.unix_timestamp(\"pick_up_time\"))\n",
    "\n",
    "    # Window and aggregate for each hack licence\n",
    "    windowedTime = df.groupBy(\"hack_licence\", F.window(\"timestamp\", window_duration)) \\\n",
    "                     .agg(\n",
    "                         F.min(\"pick_up_time\").alias(\"first_pick_up_time\"),\n",
    "                         F.max(\"drop_off_time\").alias(\"last_drop_off_time\")\n",
    "                     )\n",
    "\n",
    "    activeTimeSummary = df.groupBy(\"hack_licence\", F.window(\"timestamp\", window_duration)) \\\n",
    "                          .agg(F.sum(\"active_time\").alias(\"total_active_time\"))\n",
    "\n",
    "    # Join and calculate idle time\n",
    "    totalIdleTime = windowedTime.join(activeTimeSummary, [\"hack_licence\", \"window\"]) \\\n",
    "                                .select(\n",
    "                                    \"hack_licence\",\n",
    "                                    \"window\",\n",
    "                                    (F.unix_timestamp(\"last_drop_off_time\") - F.unix_timestamp(\"first_pick_up_time\")).alias(\"total_time\"),\n",
    "                                    \"total_active_time\",\n",
    "                                    (F.col(\"total_time\") - F.col(\"total_active_time\")).alias(\"idle_time\")\n",
    "                                ).withColumn(\"timestamp\", F.current_timestamp())\n",
    "\n",
    "    query = totalIdleTime.writeStream \\\n",
    "                         .outputMode(\"append\") \\\n",
    "                         .format(\"delta\") \\\n",
    "                         .trigger(processingTime=\"10 seconds\") \\\n",
    "                         .option(\"checkpointLocation\", f\"{checkpoint_path}/duration_{duration}_min\") \\\n",
    "                         .option(\"path\", f\"{output_path}/duration_{duration}_min\") \\\n",
    "                         .start()\n",
    "    queries.append(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88a5bafe-a7fc-4b9d-810a-7eee94624b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "for query in queries:\n",
    "    query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1ea15f2-e119-4a8a-94a6-901576e9e8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{5: Empty DataFrame\n",
      "Columns: [hack_licence, window, total_time, total_active_time, idle_time, timestamp]\n",
      "Index: [], 10: Empty DataFrame\n",
      "Columns: [hack_licence, window, total_time, total_active_time, idle_time, timestamp]\n",
      "Index: [], 15: Empty DataFrame\n",
      "Columns: [hack_licence, window, total_time, total_active_time, idle_time, timestamp]\n",
      "Index: []}\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming you have a function to read delta tables into a dataframe\n",
    "def read_delta_table(path):\n",
    "    return spark.read.format(\"delta\").load(path).toPandas()\n",
    "\n",
    "# Collect data from different window outputs\n",
    "data_frames = {duration: read_delta_table(f\"{output_path}/duration_{duration}_min\") for duration in windows}\n",
    "print(data_frames)\n",
    "\n",
    "# for duration, df in data_frames.items():\n",
    "#     df['window_size'] = f\"{duration}min\"  # Add window size as a string, e.g., '5min'\n",
    "#     df['window_start'] = pd.to_datetime(df['window'].str.extract(r'\\((.*),')[0])  # Optional: Extract start of window for any further temporal analysis\n",
    "\n",
    "# # Combine all data frames into one\n",
    "# combined_df = pd.concat(data_frames.values(), ignore_index=True)\n",
    "\n",
    "# # Drop the 'window' column if no longer needed\n",
    "# combined_df.drop('window', axis=1, inplace=True)\n",
    "\n",
    "# # Pivot the DataFrame\n",
    "# pivot_df = combined_df.pivot_table(index='hack_licence', columns='window_size', values='idle_time', aggfunc='mean')\n",
    "# print(pivot_df)\n",
    "\n",
    "# # Optional: Reset the index if you want 'hack_licence' as a regular column for plotting\n",
    "# pivot_df.reset_index(inplace=True)\n",
    "\n",
    "# # Example of plotting\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# pivot_df.set_index('hack_licence').plot(kind='bar', figsize=(10, 7))\n",
    "# plt.title('Average Idle Time per Window Size')\n",
    "# plt.xlabel('Driver')\n",
    "# plt.ylabel('Average Idle Time (seconds)')\n",
    "# plt.xticks(rotation=0)\n",
    "# plt.legend(title='Window size')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6746caef-fc7c-4d0e-98df-cdd6046393eb",
   "metadata": {},
   "source": [
    "## [Query 2] The average time it takes for a taxi to find its next fare(trip) per destination borough. This can be computed by finding the time difference, e.g. in seconds, between the trip's drop off and the next trip's pick up within a given unit of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bea404b-fc76-48f9-83d9-5946617863de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember you can register another stream\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8268c285-d55c-4be3-8e5a-e7ddebb14153",
   "metadata": {},
   "source": [
    "## [Query 3] The number of trips that started and ended within the same borough in the last hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e7ad21-59c2-4d4c-befe-9d1ceedbb74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember you can register another stream\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d0d685-c3ed-4b7d-8ebc-c3174ba55645",
   "metadata": {},
   "source": [
    "## [Query 4] The number of trips that started in one borough and ended in another one in the last hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f3578b-1960-4969-801b-adc2f45493a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
